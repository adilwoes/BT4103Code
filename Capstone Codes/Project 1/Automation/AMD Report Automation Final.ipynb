{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecda37ed",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "## There are three things needed to be in place for this file to work:\n",
    "    1. Training data for anomaly\n",
    "    2. Raw data in csv, with each slide(since it was extracted from slides) separated by columns\n",
    "    3. Download all the libraries required\n",
    "    \n",
    "## There are four files needed for this notebook to work:\n",
    "    1. \"AMD Report Automation.ipynb\", this notebook\n",
    "    2. \"constant.py\" which stores all the constant variables such as product name, technique names\n",
    "    3. \"mapping.py\" which stores all the mapping logics for columns such as product name, technique names\n",
    "    4. \"helpfunction.py\" which contains the help function\n",
    "    \n",
    "## There are three assumptions for the automation report\n",
    "    1. There are 3 slides, each slide is split to a column in raw excel.\n",
    "        1st column: powerpoint name(Strictly only file name, other contents such as filepath could lead to wrong extraction)\n",
    "        2nd column: Brief summary(analysts, requested by, etc)\n",
    "        3nd column: Everything from problem description onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774119e0",
   "metadata": {},
   "source": [
    "## Change the variables below before running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f0a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_std_df_file_name = \"Non-standardizedData.xlsx\"\n",
    "anomaly_detector_training_data = \"training_data_anomaly.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50ea30",
   "metadata": {},
   "source": [
    "## Run the codes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cce5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dateparser.search import search_dates\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jellyfish\n",
    "import datetime\n",
    "import pytz\n",
    "import json\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbd46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords ##You might need to download stopwords if you haven't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc7de8",
   "metadata": {},
   "source": [
    "## Import custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6379bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpfunction import HelpFunctions\n",
    "from constant import TEST_PHASES, TEST_PHASE_VARIANT, FAILURE, FAILURE_VARIANT, FAILURE_SUBVARIANT, FAILING_CONDITIONS, ANALYSTS, PFA_ANALYSTS, PRODUCT_NAMES, PRODUCT_EXCLUSION, TECHNIQUES, DAMAGE_INCLUSIONS\n",
    "from mapping import ANALYSTS_MAPPING, PRODUCT_MAPPING, TECHNIQUES_MAP, TECHNIQUES_MAP_LONG, TECHNIQUES_MAP_SHORT, PROD_BU_TN_MAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091556b",
   "metadata": {},
   "source": [
    "## Beginning: Reading in unstructured raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c134cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_std_df = pd.read_excel(non_std_df_file_name) ##Change file name accordingly\n",
    "rows = non_std_df.astype(str).apply(\" slide_sep \".join, axis = 1).to_list() ##Store all raw data in \"rows\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586962c",
   "metadata": {},
   "source": [
    "## 1.0 Training Anomaly Detection ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cabc1",
   "metadata": {},
   "source": [
    "### 1.1 Setting up objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917deb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "sm = SMOTE()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tfidftransformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d0b60",
   "metadata": {},
   "source": [
    "### 1.2 Reading test data(More data fed into train data, more robust the anomaly detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd3dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_detection_df = pd.read_csv(anomaly_detector_training_data)\n",
    "##Pre-processing\n",
    "anomaly_detection_df = anomaly_detection_df[~anomaly_detection_df.anomaly_detected.isna()]\n",
    "anomaly_detection_df = anomaly_detection_df[anomaly_detection_df.anomaly_detected != 0]\n",
    "anomaly_detection_df = anomaly_detection_df.drop(\"Unnamed: 0\", axis = 1)\n",
    "anomaly_detection_df.anomaly_detected = anomaly_detection_df.anomaly_detected.apply(lambda x: int(x))\n",
    "X = anomaly_detection_df.tech_used\n",
    "y = anomaly_detection_df.anomaly_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891269e",
   "metadata": {},
   "source": [
    "### 1.3 Transforming, over-sampling and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5cce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(X).toarray()\n",
    "X = tfidftransformer.fit_transform(X).toarray()\n",
    "X, y = sm.fit_resample(X, y) ##Over sampling using smote\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e81fe",
   "metadata": {},
   "source": [
    "### 1.3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd89eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for selected model:\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "while accuracy < 0.50: ##Avoid random sampling error\n",
    "    clf = MLPClassifier(solver='lbfgs', activation = \"relu\", alpha=0.1, hidden_layer_sizes=(2, 5))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy for selected model:\\n{round(accuracy, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44f20d",
   "metadata": {},
   "source": [
    "### 1.4 Wrap model into object for automation's usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66149593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyPred:\n",
    "    \n",
    "    def __init__(self, model, tfidftransformer, cv):\n",
    "        self.model = model\n",
    "        self.tfidftransformer = tfidftransformer\n",
    "        self.cv = cv\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        X = [sentence]\n",
    "        X = self.cv.transform(X).toarray()\n",
    "        X = self.tfidftransformer.transform(X)\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred[0]\n",
    "    \n",
    "anomaly_predictor = AnomalyPred(clf, tfidftransformer, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27865561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Sample Usage\n",
    "anomaly_predictor.predict('Dynamic photoemission (PEM) analysis observed missing emission sites at lower half of Channel B (mct_t1.CH0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41096b",
   "metadata": {},
   "source": [
    "## 2.0 Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5208410",
   "metadata": {},
   "source": [
    "### 2.0.1 Create Help Function Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f2bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = HelpFunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624946ec",
   "metadata": {},
   "source": [
    "### 2.0.2 Create Automation Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d88538f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardize:\n",
    "    \n",
    "    COLS =  ['job id',\n",
    "             'quarter',\n",
    "             'analyst(s)',\n",
    "             'product name',\n",
    "             'test phase',\n",
    "             'test phase variant',\n",
    "             'time',\n",
    "             'failure',\n",
    "             'failure variant',\n",
    "             'failure sub variant',\n",
    "             'failing conditions',\n",
    "             'failure remarks',\n",
    "             'fi success',\n",
    "             'pfa success',\n",
    "             'technique',\n",
    "             'fi cost',\n",
    "             'pfa cost',\n",
    "             'remarks',\n",
    "             'date finished',\n",
    "             'bu',\n",
    "             'technology node',\n",
    "             'root cause']\n",
    "    \n",
    "                ##Please concatenate last name of chinese names together. For Example, Lim Huai An -> Lim Huaian\n",
    "    \n",
    "    \n",
    "    def __init__(self, row : str, prod_name_score = 80, analyst_name_score = 65, techniques_score = 59.5, date_score = 40):\n",
    "        self.prod_name_score = prod_name_score\n",
    "        self.analyst_name_score = analyst_name_score\n",
    "        self.techniques_score = techniques_score\n",
    "        self.date_score = date_score\n",
    "        self.output = {}\n",
    "        self.original_text_chunk = row\n",
    "        self.text_chunk = self.pre_process(row)\n",
    "        self.slide0 = self.text_chunk.split(\"slide_sep\")[0]\n",
    "        self.slide1 = self.text_chunk.split(\"slide_sep\")[1]\n",
    "        self.slide2 = self.text_chunk.split(\"slide_sep\")[2] \n",
    "        self.slide0 = self.slide0.lower().replace(\":\", \" \").replace(\"'\", \" \").replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "        self.slide1 = self.slide1.lower().replace(\":\", \" \").replace(\"'\", \" \").replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "        self.slide2 = self.slide2.lower().replace(\":\", \" \").replace(\"'\", \" \").replace(\"-\", \" \")\n",
    "        ##set up constant variables\n",
    "        self.TEST_PHASES = TEST_PHASES #Variation of test phases\n",
    "        self.TEST_PHASE_VARIANT = TEST_PHASE_VARIANT #Variations of test phase variant\n",
    "        self.FAILURE =FAILURE #Variations of failures\n",
    "        self.FAILURE_VARIANT = FAILURE_VARIANT #Variations of failure variant\n",
    "        self.FAILURE_SUBVARIANT = FAILURE_SUBVARIANT #Variations of failure sub-variant\n",
    "        self.FAILING_CONDITIONS = FAILING_CONDITIONS #Variations of failing condition\n",
    "        self.ANALYSTS = ANALYSTS #List of FI analyst\n",
    "        self.PFA_ANALYSTS = PFA_ANALYSTS #List of PFA analyst\n",
    "        self.PRODUCT_NAMES = PRODUCT_NAMES #List of product names\n",
    "        self.PRODUCT_EXCLUSION = PRODUCT_EXCLUSION #List of product exclusions\n",
    "        self.TECHNIQUES = TECHNIQUES #List of techniques\n",
    "        self.ANALYSTS_MAPPING = ANALYSTS_MAPPING #Hard mapping for analysts\n",
    "        self.PRODUCT_MAPPING = PRODUCT_MAPPING #Hard mapping for product names\n",
    "        self.TECHNIQUES_MAP = TECHNIQUES_MAP #Hard mapping for techniques\n",
    "        self.TECHNIQUES_MAP_LONG = TECHNIQUES_MAP_LONG #Mapping techniques to standardised output(Long technique names)\n",
    "        self.TECHNIQUES_MAP_SHORT = TECHNIQUES_MAP_SHORT #Mapping techniques to standardised output(Short technique names)\n",
    "        self.PROD_BU_TN_MAPPING = PROD_BU_TN_MAPPING #Product BU and Technology Node mapping using product name\n",
    "        self.DAMAGE_INCLUSIONS = DAMAGE_INCLUSIONS #Words that signify FI damaged the unit during process\n",
    "        \n",
    "    \n",
    "    def test(self):\n",
    "        return self.output\n",
    "        \n",
    "    def fill(self):\n",
    "        self.fill_job_id()\n",
    "        self.fill_date_finished()\n",
    "        self.fill_analysts()\n",
    "        self.fill_product_name()\n",
    "        self.fill_test_phase()\n",
    "        self.fill_time()\n",
    "        self.fill_failure() ##consist of failure, failure variant, failure sub variant\n",
    "        self.fill_failure_condition()\n",
    "        self.fill_failure_remark()\n",
    "        self.fill_techniques()\n",
    "        self.fill_root_cause()\n",
    "        self.fill_fi_success()\n",
    "        self.fill_fi_cost()\n",
    "        self.fill_remark()\n",
    "        self.fill_pfa_cost()\n",
    "        self.fill_pfa_success()\n",
    "        self.output = dict([(k, self.output[k]) for k in self.COLS]) ##Align the columns with self.COLS\n",
    "        self.output[\"original_text\"] = self.original_text_chunk\n",
    "    \n",
    "    def pre_process(self, row):\n",
    "        \"\"\"\n",
    "        1. Change all content to lower case\n",
    "        2. Add space to all commas to improve tokenization\n",
    "        3. Remove '\\n' and other symbols using ReGEx\n",
    "        4. Tokenise words using Textblob\n",
    "        5. Join the tokenised words into a string and remove all quotation marks\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        row: str\n",
    "            The row of data from Excel that is used for data extraction. It is assumed that each row\n",
    "            is extracted from power point slides and each slide is separated using 'slide_sep' separator.\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        row:\n",
    "            Pre-processed row data.\n",
    "        \"\"\"\n",
    "        row = row.lower()\n",
    "        row = row.replace(\",\", \", \")\n",
    "        row = re.sub(r'[^\\w\\s]',' ',row)\n",
    "        row = TextBlob(row)\n",
    "        row = row.words\n",
    "        row = \" \".join(row).replace(\"'\", \"\").replace('\"', \"\")\n",
    "        return row\n",
    "    \n",
    "    def fill_job_id(self):\n",
    "        \"\"\"\n",
    "        Using simple regex pattern to extract job id\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"job id\"] dictionary.\n",
    "            \n",
    "        \"\"\"\n",
    "        job_id = re.search(\"[a-z]{2}\\d{7}[a-z]{2}\", self.text_chunk)[0]\n",
    "        self.output[\"job id\"] = job_id\n",
    "        ##self.text_chunk = self.text_chunk[self.text_chunk.find(job_id):]\n",
    "        pass\n",
    "    \n",
    "    def fill_date_finished(self):\n",
    "        \"\"\"\n",
    "        Extract date from the text chunk. \n",
    "        Using fuzzy ratio to further enhance the accuracy and validity of date extracted. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"date finished\"] and self.output[\"quarter\"].\n",
    "        \"\"\"\n",
    "        text = self.text_chunk\n",
    "        min_date = datetime.datetime(2010, 1, 1)\n",
    "        original_text = self.original_text_chunk.replace(\"\\n\", \"\")\n",
    "        # Standardizing timezone\n",
    "        k = search_dates(f\"{text} {original_text}\", languages = [\"en\"], settings={'TIMEZONE': 'UTC'})\n",
    "        k = list(map(lambda x: (x[0], x[1].replace(tzinfo=pytz.utc)), k)) \n",
    "        k = list(filter(lambda x: x[1] > pytz.timezone(\"UTC\").localize(min_date) and x[1] < datetime.datetime.now(datetime.timezone.utc), k))\n",
    "        # Using fuzzy ratio to enhance date search\n",
    "        k = list(map(lambda x: (x[0], x[1], fuzz.ratio(x[1].strftime('%m%d%Y'), x[0])), k))\n",
    "        k = list(filter(lambda x: x[2] > self.date_score, k))\n",
    "        k = list(filter(lambda x: len(x[0]) >= 6, k))\n",
    "        if k:\n",
    "            ##Restrict date to 2010 till present\n",
    "            k.sort(key = lambda x: x[2], reverse = True)\n",
    "            date = k[0][1]\n",
    "            quarter = math.ceil(date.month/3.)\n",
    "            self.output[\"date finished\"] = date.strftime(\"%m/%d/%Y\")\n",
    "            self.output[\"quarter\"] = f\"Q{quarter}\"\n",
    "        else:\n",
    "            self.output[\"date finished\"] = \"00/00/00\"\n",
    "            self.output[\"quarter\"] = \"0\"\n",
    "        pass\n",
    "\n",
    "    def fill_analysts(self):\n",
    "        \"\"\"\n",
    "        Obtain analyst names that appear in this job using following steps:\n",
    "        1. Find range. Range is defined content between the word 'analyst' and 'review'.\n",
    "        2. Removed all contents that we know do not belong to FI analyst names. \n",
    "            eg. PFA names\n",
    "        3. Apply hard mapping. \n",
    "            eg. gopi -> gopinath\n",
    "        4. Search logic\n",
    "            a. For each analyst name, we split search content into list, and concatenate element i and i + 1 to increase\n",
    "            search variability. (Since sometimes analyst names might split)\n",
    "            b. Define analyst name as arrays of their first and last names. Eg. David Jinjie -> [0, 0]\n",
    "            c. For each component of analyst name, find the best match. Eg. Find best match of 'david'. If it is \n",
    "            present, update the respective analyst name array with 1.\n",
    "            d. After iterating through David and Jinjie, if analyst array has at least one '1', consider it as analyst\n",
    "            name is present in the content.\n",
    "            \n",
    "        *Note, we do not consider analyst name to be a match if the component of the name has only two letters. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"analyst(s)\"]\n",
    "        \"\"\"\n",
    "        # Define range\n",
    "        texts = self.text_chunk[abs(self.text_chunk.find(\"analyst\") - 1):self.text_chunk.find(\"review\")]\n",
    "        for pfa in self.PFA_ANALYSTS:# Remove PFA analyst, aka removing names that we know should not belong to FI \n",
    "            texts = texts.replace(pfa, \"\")\n",
    "        for pattern, mapping in self.ANALYSTS_MAPPING.items(): # Apply mapping\n",
    "            texts = texts.replace(pattern, mapping)\n",
    "        result = []\n",
    "        for analyst in self.ANALYSTS:\n",
    "            name = analyst.split(\" \")\n",
    "            name_arr = [0] * len(name) # Create list for name, EG \"David Jinjie\" -> [0, 0]\n",
    "            i = 0\n",
    "            temp_texts = texts.split(\" \")\n",
    "            temp_texts.extend([f\"{temp_texts[i]}{temp_texts[i + 1]}\" for i in range(len(temp_texts) - 1)]) # concat with next string\n",
    "            while len(temp_texts) > 1 and i < len(name_arr): \n",
    "                n = name[i]\n",
    "                best_match = hf.find_similar_substring(temp_texts, n)\n",
    "                if hf.is_abbreviation(best_match, n) and fuzz.ratio(n, best_match) > self.analyst_name_score and len(best_match) > 2:\n",
    "                    best_idx = temp_texts.index(best_match) \n",
    "                    name_arr[i] = best_idx\n",
    "                    if len(n) < len(best_match): # Allows for partial name match\n",
    "                        temp_texts[best_idx] = best_match[:len(n)] # If it is partial match, do not look ahead.\n",
    "                    else:\n",
    "                        temp_texts = temp_texts[best_idx + 1:] # If it is full match, look ahead.\n",
    "                i += 1\n",
    "            if name_arr.count(0) <= 1: # At least one word from the name must be present.\n",
    "                result.append(analyst)\n",
    "        self.output[\"analyst(s)\"] = \", \".join(result)\n",
    "\n",
    "    def fill_product_name(self):\n",
    "        \"\"\"\n",
    "        Using similarity score to pin-point product name.\n",
    "        1. Split search range into slides. Since product names are usually in the first slide, we relax the \n",
    "        condition of matching for first slide.\n",
    "        2. Remove all words that we know do not belong to product name\n",
    "            eg. Failures such as 'bist'\n",
    "        3. Apply hard mapping. \n",
    "        4. Search Logic:\n",
    "            a. For each slide, split into list of texts. Concatenate element i with i + 1 to increase search variability.\n",
    "            b. For each product names in product list, look for best maching product name. \n",
    "            c. If the best match is an abbreviation and the match score is higher than user defined score, add it to result.\n",
    "        5. If there is result in the search, take the best result and update dictionary.\n",
    "        6. Update BU and technology node accordingly.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"product name\"]\n",
    "\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        slides = self.text_chunk.replace(self.output[\"job id\"], \"\") # remove job id\n",
    "        for exclusion in self.FAILURE + self.PRODUCT_EXCLUSION:\n",
    "            slides = slides.replace(exclusion, \"\") # Remove all the failure words\n",
    "        for k, v in self.PRODUCT_MAPPING.items():\n",
    "            slides = slides.replace(k, v)\n",
    "        slides = slides.split(\"slide_sep\")\n",
    "        for j in range(len(slides[:2])):\n",
    "            slide = slides[j]\n",
    "            texts = re.split(\"_| \", slide)\n",
    "            texts = [t for t in texts if len(t) > 0] # remove empty strings\n",
    "            texts.extend([f\"{texts[i]} {texts[i + 1]}\" for i in range(len(texts) - 1)]) # concat with next string(two fold)\n",
    "            for p in self.PRODUCT_NAMES:\n",
    "                best_matches = process.extract(p, texts, scorer = fuzz.ratio)\n",
    "                if j == 0:\n",
    "                    best_matches = best_matches[:1] # Take the first product if product is in first slide.\n",
    "                for best_match in best_matches:\n",
    "                    score = best_match[1]\n",
    "                    best_match = best_match[0]\n",
    "                    if hf.is_abbreviation(best_match, p) and score > self.prod_name_score * (j if j == 0 else 1): # Gives penalty to slide 2\n",
    "                        result.append((p, score, best_match))\n",
    "            if result:\n",
    "                result.sort(key = lambda x: x[1] + len(x[0]), reverse = True) # return longer match\n",
    "                self.output[\"product name\"] = result[0][0]\n",
    "                prod_name = self.output[\"product name\"]\n",
    "                if prod_name in self.PROD_BU_TN_MAPPING.keys():\n",
    "                    bu = self.PROD_BU_TN_MAPPING[prod_name][0]\n",
    "                    tn = self.PROD_BU_TN_MAPPING[prod_name][1]\n",
    "                    self.output['bu'] = bu\n",
    "                    self.output['technology node'] = tn\n",
    "                else:\n",
    "                    self.output['bu'] = \"no bu found\"\n",
    "                    self.output['technology node'] = \"no tn found\"\n",
    "                return # If result is found at earlier slide terminate the search.\n",
    "        self.output[\"product name\"] = \"no product found\"\n",
    "        self.output['bu'] = \"no product found\"\n",
    "        self.output['technology node'] = \"no product found\"\n",
    "        return\n",
    "        ##Assume only one product per row\n",
    "\n",
    "\n",
    "    def fill_test_phase(self):\n",
    "        \"\"\"\n",
    "        Extract test phase from the text chunk. \n",
    "        Search for test phase first, then search for test phase variant through exact matching\n",
    "        since each category have limited characters.\n",
    "        As most of the time, only the test phase variant is mentioned for 'REL' test phase,\n",
    "        if test phase isn't found but test phase variant is found, re-fill test phase accordingly.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"test phase\"] and self.output[\"test phase variant\"]\n",
    "        \"\"\"\n",
    "        test_phase = \"\"\n",
    "        test_phase_variant = \"\"\n",
    "        for test in self.TEST_PHASES:\n",
    "            if (test in self.slide0) or (test in self.slide1):\n",
    "                test_phase = test\n",
    "                break\n",
    "\n",
    "        if test_phase == \"\":\n",
    "            for test in self.TEST_PHASES:\n",
    "                if test in self.slide2:\n",
    "                    test_phase = test\n",
    "                    break\n",
    "                    \n",
    "        if test_phase == \"\" or test_phase=='rel':\n",
    "            if \"tigershark\" in self.slide1.split():\n",
    "                test_phase_variant = \"era\"\n",
    "                test_phase = \"rel\"\n",
    "            elif any(word.startswith(\"era\") for word in self.slide1.split()):\n",
    "                test_phase_variant = \"era\"\n",
    "                test_phase = \"rel\"\n",
    "            elif any(word.startswith(\"qual\") for word in self.slide1.split()):\n",
    "                test_phase_variant = \"qual\"\n",
    "                test_phase = \"rel\"\n",
    "            else:\n",
    "                for variant in self.TEST_PHASE_VARIANT:\n",
    "                    if variant in self.slide1.split(): \n",
    "                        test_phase_variant = variant\n",
    "                        test_phase = \"rel\" \n",
    "                        break\n",
    "\n",
    "        if test_phase_variant ==\"\" and (test_phase == \"\" or test_phase=='rel'):\n",
    "            for variant in self.TEST_PHASE_VARIANT:\n",
    "                if variant in self.slide2.split(): \n",
    "                    test_phase_variant = variant\n",
    "                    test_phase = \"rel\" \n",
    "                    break\n",
    "        self.output[\"test phase\"] = test_phase.upper() if test_phase else \"\"\n",
    "        self.output[\"test phase variant\"] = test_phase_variant.upper() if test_phase_variant else \"\"\n",
    "\n",
    "\n",
    "    \n",
    "    def fill_time(self):\n",
    "        \"\"\"\n",
    "        Use regex pattern to extract time.\n",
    "        Note: Only has value when test_phase == 'rel'\n",
    "        Hence, we search for time when test phase is rel or has not been found.\n",
    "        If test phase is nout found previously but there is time,\n",
    "        then we indicate test phase as 'rel'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"time\"] and self.output[\"test_phase\"]\n",
    "            \n",
    "        \"\"\"\n",
    "        time = \"\"\n",
    "        if self.output[\"test phase\"] == 'rel' or self.output[\"test phase\"] == '':\n",
    "            result = re.findall(r'\\bt\\d*\\.?\\d+h?\\b', self.slide1)\n",
    "            result += re.findall(r'\\bt\\d*\\.?\\d+h?\\b', self.slide2)\n",
    "            if result:\n",
    "                time = result[0].upper()\n",
    "                self.output[\"test phase\"] = 'rel'\n",
    "                if time[-1]!='H':\n",
    "                    time += 'H'\n",
    "        self.output[\"time\"] = time if time else \"\"\n",
    "\n",
    "    \n",
    "    # helper function for fill_failure function\n",
    "    def helper(f, failure):\n",
    "        if f == 'parameter' or f=='psshort' or f=='short' or f=='leak':\n",
    "            if failure == '':\n",
    "                failure = 'parametric'\n",
    "            else:\n",
    "                failure += \", \" + 'parametric'\n",
    "        elif f == 'spec':\n",
    "            if failure == '':\n",
    "                failure = 'iospec'\n",
    "            else:\n",
    "                failure += \", \" + 'iospec'\n",
    "        elif f == 'scandelay':\n",
    "            if failure == '':\n",
    "                failure = 'scan'\n",
    "            else:\n",
    "                failure += \", \" + 'scan'\n",
    "        else:\n",
    "            if failure == '':\n",
    "                failure = f\n",
    "            else:\n",
    "                failure += \", \" + f\n",
    "\n",
    "    # helper function for fill_failure function\n",
    "    def helper(self, f, failure):\n",
    "        if f == 'parameter' or f=='psshort' or f=='short' or f=='leak':\n",
    "            if failure == '':\n",
    "                failure = 'parametric'\n",
    "            else:\n",
    "                failure += \", \" + 'parametric'\n",
    "        elif f == 'spec':\n",
    "            if failure == '':\n",
    "                failure = 'iospec'\n",
    "            else:\n",
    "                failure += \", \" + 'iospec'\n",
    "        elif f == 'scandelay':\n",
    "            if failure == '':\n",
    "                failure = 'scan'\n",
    "            else:\n",
    "                failure += \", \" + 'scan'\n",
    "        else:\n",
    "            if failure == '':\n",
    "                failure = f\n",
    "            else:\n",
    "                failure += \", \" + f\n",
    "        return failure\n",
    "    \n",
    "    def fill_failure(self):\n",
    "        \"\"\"\n",
    "        Fill failure, failure variant and failure sub variant accordingly \n",
    "        by searching for exact match (since each category have limited characters)\n",
    "        with the help of mapping list which specifies the relationship between them.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"failure\"], self.output[\"failure variant\"] and self.output[\"failure sub variant\"] \n",
    "        \"\"\"\n",
    "        failure = ''\n",
    "        failure_variant = ''\n",
    "        failure_sub_variant = ''\n",
    "\n",
    "\n",
    "        for f in self.FAILURE:\n",
    "            if f in self.slide0.split():\n",
    "                failure = self.helper(f, failure)\n",
    "                break\n",
    "\n",
    "        # to fill failure \n",
    "        if failure == \"\":\n",
    "            for f in self.FAILURE:\n",
    "                if f in self.slide1.split():\n",
    "                    failure = self.helper(f, failure)\n",
    "                    break\n",
    "\n",
    "        if failure == \"\":\n",
    "            for f in self.FAILURE:\n",
    "                if f in self.slide2.split():\n",
    "                    failure = self.helper(f, failure)\n",
    "                    break\n",
    "\n",
    "\n",
    "        # to fill failure variant\n",
    "        failure_lst = failure.split(\", \")\n",
    "        for f in failure_lst:\n",
    "            if f in list(self.FAILURE_VARIANT.keys()): \n",
    "                for v in self.FAILURE_VARIANT[failure]:\n",
    "                    if v in self.slide2.split():\n",
    "                        if failure_sub_variant == \"\":\n",
    "                            failure_sub_variant = v\n",
    "                        else:\n",
    "                            failure_sub_variant += \", \" + v\n",
    "\n",
    "        # to fill failure sub variant \n",
    "        for f in failure_lst:\n",
    "            if failure in list(self.FAILURE_SUBVARIANT.keys()):\n",
    "                for v in self.FAILURE_SUBVARIANT[failure]:\n",
    "                    if v in self.slide2.split():\n",
    "                        if failure_sub_variant == \"\":\n",
    "                            failure_sub_variant = v\n",
    "                        else:\n",
    "                            failure_sub_variant += \", \" + v\n",
    "\n",
    "\n",
    "        self.output[\"failure\"] = failure if failure else \"\"\n",
    "        self.output[\"failure variant\"] = failure_variant if failure_variant else \"\"\n",
    "        self.output[\"failure sub variant\"] = failure_sub_variant if failure_sub_variant else \"\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fill_failure_condition(self):\n",
    "        \"\"\"\n",
    "        Fill failing condition by searching for exact match since each category have limited characters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"failure_condition\"] \n",
    "        \"\"\"\n",
    "        failing_conditions = \"\"\n",
    "        for fc in self.FAILING_CONDITIONS:\n",
    "            if fc in self.slide2.split():\n",
    "                if failing_conditions == \"\":\n",
    "                    failing_conditions = fc\n",
    "                else:\n",
    "                    failing_conditions += \", \" + fc\n",
    "        self.output[\"failing conditions\"] = failing_conditions if failing_conditions else \"\"\n",
    "\n",
    "\n",
    "    def fill_failure_remark(self):\n",
    "        self.output[\"failure remarks\"] = \"\"\n",
    "        pass\n",
    "    \n",
    "    def fill_fi_success(self):\n",
    "        \"\"\"\n",
    "        Define FI success as if both conditions are met:\n",
    "        1. Product is not damaged\n",
    "        2. Anomaly is detected by FI techniques\n",
    "        \n",
    "        Note*: We only consider product to be damaged during investigation if words in DAMAGE_INCLUSION appears\n",
    "        in the content.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"FI success\"]\n",
    "        \"\"\"\n",
    "        if \"yes\" not in str(self.output[\"technique\"]).lower() and \"damage\" in self.output[\"root cause\"]:\n",
    "            for inclusion in self.DAMAGE_INCLUSIONS: ## Since there is word damage, we want to see if damage is from FI or customer\n",
    "                if inclusion in str(self.output[\"root cause\"]).lower().replace(\" \", \"\"):\n",
    "                    self.output[\"fi success\"] = 0\n",
    "                    return \n",
    "        if \"yes\" not in str(self.output[\"technique\"]).lower():\n",
    "            self.output[\"fi success\"] = 0\n",
    "        else:\n",
    "            self.output[\"fi success\"] = 1\n",
    "            \n",
    "    def fill_fi_cost(self):\n",
    "        self.output[\"fi cost\"] = \"\"\n",
    "        pass\n",
    "        \n",
    "    def fill_pfa_cost(self):\n",
    "        self.output[\"pfa cost\"] = \"\"\n",
    "        pass\n",
    "    \n",
    "    def fill_pfa_success(self):\n",
    "        self.output[\"pfa success\"] = \"\"\n",
    "        pass\n",
    "    \n",
    "    def fill_remark(self):\n",
    "        self.output[\"remarks\"] = \"\"\n",
    "        pass\n",
    "    \n",
    "    def fill_techniques(self):\n",
    "        \"\"\"\n",
    "        This function defined a customized scorer for most similar words extraction. The scorer gives extract\n",
    "        credit to words that seem to be exact abbreviation. (Eg. PEM = Photon Emission Microscopy, where first 3 letters\n",
    "        of the full name is exactly equal to its abbreviation)\n",
    "        \n",
    "        1. Define the range for search. Try to minimise the search range as much as possible.\n",
    "        2. Search range is the original text chunk instead. The processed text chunk do not contain line breaks, \n",
    "        however, we assume each line break contains exactly one technique. \n",
    "        3. Apply hard mapping.\n",
    "        4. Split the search content into arrays, and pre-process each element in the array.\n",
    "        5. Concatenate element i with i + 1 and i + 2 to increase variability in the search range.\n",
    "        6. Search Logic\n",
    "            a. For each row in the search content, find the most matching technique.\n",
    "            b. Find the best matching technique after looking through all techniques\n",
    "            c. Each technique is default dynamic, and default low resolution. \n",
    "            d. If high resolution detects anomaly, add low resolution technique and say that it detected anomaly too.\n",
    "        \n",
    "        Note*: Only consider the matched word if the it has at more than 2 letters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"techniques\"]\n",
    "        \"\"\"\n",
    "        def MyScorer(s1, s2): # Define scorer\n",
    "            fratio = fuzz.ratio(s1, s2)\n",
    "            if len(s1) > len(s2):\n",
    "                longer = s1\n",
    "                shorter = s2\n",
    "            else:\n",
    "                longer = s2\n",
    "                shorter = s1\n",
    "            if shorter == \"\".join(list(map(lambda x: x[0], longer.split(\" \")))):\n",
    "                fratio += 100\n",
    "                fratio = fratio/2\n",
    "            return fratio\n",
    "        result = []\n",
    "        text_chunk = self.original_text_chunk.lower() # Bring to lower case\n",
    "        if text_chunk.find(\"analysis summary\") > 0:\n",
    "            idx = text_chunk.find(\"analysis summary\")\n",
    "        elif text_chunk.find(\"analysis\") > 0:\n",
    "            idx = text_chunk.find(\"analysis\")\n",
    "        elif text_chunk.find(\"summary\") > 0:\n",
    "            idx = text_chunk.find(\"summary\")\n",
    "        else:\n",
    "            idx = 0\n",
    "        for technique, mapping in self.TECHNIQUES_MAP.items():\n",
    "            text_chunk = text_chunk.replace(technique, mapping)\n",
    "        text_chunk = re.split(r\"\\\\.|\\n\", text_chunk[idx:])\n",
    "        text_chunk = [self.pre_process(row) for row in text_chunk]    \n",
    "        for j in range(len(text_chunk)):\n",
    "            sentence = text_chunk[j]\n",
    "            texts = sentence.split(\" \")\n",
    "            two_fold = [f\"{texts[i]} {texts[i + 1]}\" for i in range(len(texts) - 1)]\n",
    "            three_fold = [f\"{texts[i]} {texts[i + 1]} {texts[i + 2]}\" for i in range(len(texts) - 2)]\n",
    "            texts.extend(two_fold) # concat with next string\n",
    "            texts.extend(three_fold) # two fold/three fold allows matching names that were separated\n",
    "            temp_result = []\n",
    "            for t in self.TECHNIQUES:\n",
    "                best_matches = process.extract(t, texts, scorer = MyScorer)\n",
    "                best_matches = list(filter(lambda x: hf.is_abbreviation(x[0], t), best_matches))\n",
    "                for best_match in best_matches[:3]: ##Check top 3\n",
    "                    temp_result.append((best_match[0], best_match[1], t))\n",
    "            temp_result.sort(key = lambda x: x[1], reverse = True)\n",
    "            if not temp_result:\n",
    "                continue\n",
    "            best_match, score, t = temp_result[0]\n",
    "            if hf.is_abbreviation(best_match, t) and score > self.techniques_score and len(best_match) > 2: \n",
    "                is_high_res = \"High Res\" if \"highres\" in sentence.replace(\" \", \"\") else \"Low Res\"\n",
    "                is_static = \"Static\" if \"static\" in sentence else \"Dynamic\"\n",
    "                detector = \"Yes\" if anomaly_predictor.predict(sentence) == 1 else \"No\"\n",
    "                result.append((is_static, is_high_res, t, detector, score))\n",
    "                if is_high_res == \"High Res\": # If high res is performed, means low res was performed without anomaly detection\n",
    "                    result.append((is_static, \"Low Res\", t, \"Yes\", score))\n",
    "        result.append((\"\", \"\", \"layout tracing\", \"No\", 0))\n",
    "        if result: ##Fill up results\n",
    "            result.sort(key = lambda x: x[-1], reverse = True)\n",
    "            o = {}\n",
    "            for r in result:\n",
    "                if r[2] in self.TECHNIQUES_MAP_LONG.keys(): # Apply output mapping for long and short techniques(with and without dyanmic/static high res/low res)\n",
    "                    name = f\"{r[0]} {r[1]} {self.TECHNIQUES_MAP_LONG[r[2]]}\"\n",
    "                elif r[2] in self.TECHNIQUES_MAP_SHORT.keys():\n",
    "                    name = f\"{self.TECHNIQUES_MAP_SHORT[r[2]]}\"\n",
    "                if name not in o.keys(): \n",
    "                    o[name] = [r[3], \"\"]\n",
    "                elif o[name] != [\"No\", \"\"]:\n",
    "                    o[name] = [r[3], \"\"]\n",
    "                else:\n",
    "                    continue\n",
    "            self.output[\"technique\"] = str(o)\n",
    "        else:\n",
    "            self.output[\"technique\"] = \"no techniques found\"\n",
    "    \n",
    "    def fill_root_cause(self):\n",
    "        \"\"\"\n",
    "        Since machine is unable to identify the root cause in the content, we can only assume that all \n",
    "        the contents after the word 'conclusion' is the root cause.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Nil\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        None:\n",
    "            Update self.output[\"root cuase\"]\n",
    "        \"\"\"\n",
    "        s = self.original_text_chunk\n",
    "        if \"conclusion\" in s.lower(): # if the word conclusion is in the content, return everything that is after it.\n",
    "            c = s[s.lower().find(\"conclusion\") + len(\"conclusion\"):]\n",
    "            self.output[\"root cause\"] = self.pre_process(c).replace(\"executive summary\", \"\").strip()\n",
    "            return\n",
    "        self.output[\"root cause\"] = self.original_text_chunk # Else, output the entire row. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfcd4a",
   "metadata": {},
   "source": [
    "### 2.1 Execute automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38ecded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for row in rows:\n",
    "    try:\n",
    "        s = Standardize(row)\n",
    "        s.fill()\n",
    "        d2 = s.test()\n",
    "        if not result.keys():\n",
    "            result = d2\n",
    "            continue\n",
    "        result = {key:np.hstack([result[key],d2[key]]) for key in d2.keys()}\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(f\"Unable to extract:\\n{row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "32094c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = pd.DataFrame(result).groupby([\"job id\"], as_index = False).agg(lambda x: \", \".join([str(x) for x in list(set(x))]))\n",
    "output = pd.DataFrame(result)\n",
    "output[output[\"analyst(s)\"] != \"\"].to_csv(\"automation_output_v2_fi_only.csv\")\n",
    "output.to_csv(\"automation_output_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d5db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
